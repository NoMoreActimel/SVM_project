{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # reading data from csv\n",
    "import statsmodels.api as sm # finding the p-value\n",
    "from scipy.stats import pearsonr # correlation and p-value\n",
    "from sklearn.preprocessing import MinMaxScaler # normalization\n",
    "from sklearn.model_selection import train_test_split # splitting dataframes\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score # result estimation\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import svm # comparing to sklearn SVM\n",
    "\n",
    "\n",
    "from scipy.optimize import Bounds, BFGS\n",
    "from scipy.optimize import LinearConstraint, minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lagrange_dual(c, X, y):\n",
    "    significant_ind = np.where(c > 1e-7)[0]\n",
    "    f = 0\n",
    "    for i in significant_ind:\n",
    "        for j in significant_ind:\n",
    "            f -= y[i] * y[j] * c[i] * c[j] * np.dot(X[i], X[j])\n",
    "    f = sum(c) + f / 2\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def fit(self, X_train, y_train, lambda_=0.1, learning_rate=0.1, print_info=False):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        self.samples = self.X_train.shape[0]\n",
    "        self.features = self.X_train.shape[1]\n",
    "\n",
    "        self.w = np.zeros(self.features).reshape((self.features, 1))\n",
    "        self.lambda_ = lambda_\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.sgd(print_info)\n",
    "        # self.optimize_dual_with_w()\n",
    "        return self.w\n",
    "\n",
    "    # MODEL TRAINING\n",
    "    def compute_cost(self, X, y):\n",
    "        hinge_loss = 0\n",
    "        for i in range(len(y)):\n",
    "            hinge_loss += max(0, 1 - y[i] * (X[i] @ self.w))\n",
    "        hinge_loss /= self.features\n",
    "\n",
    "        # print(hinge_loss, hinge_loss_)\n",
    "\n",
    "        return 0.5 * self.lambda_ * (self.w.T @ self.w) + hinge_loss\n",
    "\n",
    "\n",
    "    def calculate_cost_gradient(self, X_batch, y_batch):\n",
    "        if type(y_batch) is type(np.float64()):\n",
    "            y_batch = np.array([y_batch]).reshape((1, 1))\n",
    "            X_batch = np.array([X_batch]).reshape((self.features, 1))\n",
    "            # print(\"reshaped batches shapes:\", X_batch.shape, y_batch.shape)\n",
    "\n",
    "        distances = 1 - y_batch * (X_batch.T @ self.w)\n",
    "        dw = np.zeros(self.features).reshape((self.features, 1))\n",
    "\n",
    "        v = X_batch @ y_batch\n",
    "        for i in range(len(distances)):\n",
    "            dw += self.lambda_ * self.w - ((distances[i] > 0) * v).reshape((self.features, 1))\n",
    "\n",
    "        dw /= self.features\n",
    "        return dw\n",
    "    \n",
    "        \n",
    "    def sgd(self, print_info=False):\n",
    "        iterations = 1000\n",
    "        cost_threshold_multiplier = 0.01\n",
    "\n",
    "        cur_pow = 0\n",
    "        prev_cost = 0\n",
    "        for iteration in range(iterations):\n",
    "            delta = 0\n",
    "            for i in np.random.permutation(self.X_train.shape[0]):\n",
    "                delta = self.calculate_cost_gradient(self.X_train[i].T, self.y_train[i])\n",
    "                self.w = self.w - self.learning_rate * delta\n",
    "                \n",
    "            if print_info and (iteration == 2 ** cur_pow or iteration == iterations - 1):\n",
    "                cost = self.compute_cost(self.X_train, self.y_train)\n",
    "                print(\"iteration no. %.6f, cost = %.6f, delta[0] = %.6f, w[0] = %.6f\" % \n",
    "                    (iteration, cost, self.learning_rate * delta[0], self.w[0]))\n",
    "\n",
    "                if abs(cost - prev_cost) < cost_threshold_multiplier * prev_cost:\n",
    "                    break\n",
    "                cur_pow += 1\n",
    "                prev_cost = cost\n",
    "    \n",
    "\n",
    "    def optimize_dual_coeffs(self):\n",
    "        c_0 = np.random.rand(self.samples) / (2 * self.features * self.lambda_)\n",
    "\n",
    "        linear_constraint = LinearConstraint(y, [0], [0])\n",
    "        bounds_for_c = Bounds(np.zeros(self.samples), np.full(self.samples, 1 / (2 * self.features * self.lambda_)))\n",
    "\n",
    "        optimized = minimize(Lagrange_dual, c_0, args=(self.X_train, self.y_train), method='trust-constr',\n",
    "            hess=BFGS(), constraints=[linear_constraint], bounds=bounds_for_c)\n",
    "        print(1)\n",
    "        self.c = optimized.x\n",
    "        return self.c\n",
    "    \n",
    "    def optimize_dual_with_w(self):\n",
    "        self.optimize_dual_coeffs()\n",
    "        self.w = self.X_train @ (self.c * self.y_train)\n",
    "        return self.w\n",
    "\n",
    "            \n",
    "    def test_classifier(self, X_test, y_test):\n",
    "        y_test_predicted = np.sign(X_test @ self.w)\n",
    "\n",
    "        print(\"accuracy on test dataset: %.6f\" % accuracy_score(y_test, y_test_predicted))\n",
    "        print(\"recall on test dataset: %.6f\" % recall_score(y_test, y_test_predicted))\n",
    "        print(\"precision on test dataset: %.6f\" % precision_score(y_test, y_test_predicted))\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        y_test_predicted = np.sign(X_test @ self.w)\n",
    "        return accuracy_score(y_test, y_test_predicted)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE REDUNDANT FEATURES\n",
    "\n",
    "def remove_correlated_features(X_df, correlation_threshold):\n",
    "    features = X_df.shape[1]\n",
    "    # correlations between X_df columns (features)\n",
    "    correlations = X_df.corr().abs().to_numpy()\n",
    "    columns_dropped = np.zeros(features)\n",
    "\n",
    "    for i in range(features):\n",
    "        for j in range(i + 1, features):\n",
    "            if correlations[i, j] >= correlation_threshold:\n",
    "                columns_dropped[i] = 1\n",
    "                break\n",
    "    \n",
    "    features_dropped = X_df.columns[columns_dropped]\n",
    "    X_df.drop(columns=features_dropped, inplace=True)\n",
    "\n",
    "\n",
    "def remove_less_significant_features(X_df, y_df):\n",
    "    sl = 0.05\n",
    "    regression_ols = None\n",
    "    columns_dropped = np.array([])\n",
    "    for itr in range(0, len(X_df.columns)):\n",
    "        regression_ols = sm.OLS(y_df, X_df).fit()\n",
    "        max_col = regression_ols.pvalues.idxmax()\n",
    "        max_val = regression_ols.pvalues.max()\n",
    "        if max_val > sl:\n",
    "            X_df.drop(max_col, axis='columns', inplace=True)\n",
    "            columns_dropped = np.append(columns_dropped, [max_col])\n",
    "        else:\n",
    "            break\n",
    "    regression_ols.summary()\n",
    "    return columns_dropped\n",
    "\n",
    "def remove_correlated_and_insignificant_features(X_df, correlation_threshold, p_value_threshold):\n",
    "    features = X_df.shape[1]\n",
    "\n",
    "    columns_dropped = np.full(features, False, dtype=bool)\n",
    "\n",
    "    for i in range(features):\n",
    "        for j in range(i + 1, features):\n",
    "            corr, p_value = pearsonr(X_df.iloc[:, i], X_df.iloc[:, j])\n",
    "            if corr >= correlation_threshold or p_value >= p_value_threshold:\n",
    "                columns_dropped[i] = True\n",
    "    \n",
    "    print(sum(columns_dropped))\n",
    "    features_dropped = X_df.columns[columns_dropped]\n",
    "    X_df.drop(columns=features_dropped, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('./data.csv') \n",
    "\n",
    "diagnosis_map = {'M':1, 'B':-1}\n",
    "data['diagnosis'] = data['diagnosis'].map(diagnosis_map)\n",
    "\n",
    "data.drop(data.columns[[-1, 0]], axis=1, inplace=True)\n",
    "\n",
    "X_df_ = data.iloc[:, 1:].dropna() # features\n",
    "y_df = data.loc[:, 'diagnosis'].dropna() # labels\n",
    "\n",
    "# remove_correlated_features(X_df_, 0.95)\n",
    "# remove_less_significant_features(X_df_, y_df)\n",
    "remove_correlated_and_insignificant_features(X_df_, 0.95, 0.1)\n",
    "\n",
    "X_df = pd.DataFrame(MinMaxScaler().fit_transform(X_df_.values)) # normalized features\n",
    "X_df.insert(loc=X_df.shape[1], column='for w0', value=1)\n",
    "\n",
    "X = X_df.to_numpy(dtype=np.float64())\n",
    "y = y_df.to_numpy(dtype=np.float64())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "clf = SVM()\n",
    "# w = clf.fit(X_train, y_train, lambda_=0.0001, learning_rate=0.01, print_info=False)\n",
    "# clf.test_classifier(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n",
      "0.9649122807017544\n",
      "0.9473684210526315\n",
      "0.9385964912280702\n",
      "0.956140350877193\n",
      "average score: 0.956140350877193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn import svm\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# clf_sklearn_svm = svm.SVC(kernel=\"linear\", C=1).fit(X_train, y_train)\n",
    "# clf_sklearn_svm.score(X_test, y_test)\n",
    "\n",
    "def cross_validation(clf, X, y, n_splits=5, test_size=0.2, custom_classifier=True):\n",
    "    cv = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=4)\n",
    "    \n",
    "    avg_score = 0\n",
    "    for train_indices, test_indices in cv.split(X):\n",
    "        X_train, y_train = X[train_indices, :], y[train_indices]\n",
    "        X_test, y_test = X[test_indices, :], y[test_indices]\n",
    "\n",
    "        if custom_classifier:\n",
    "            clf.fit(X_train, y_train, lambda_=0.0001, learning_rate=0.01)\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "        cur_score = clf.score(X_test, y_test)\n",
    "        print(cur_score)\n",
    "        avg_score += cur_score\n",
    "\n",
    "    avg_score /= n_splits\n",
    "    print(\"average score:\", avg_score)\n",
    "    return avg_score\n",
    "\n",
    "cross_validation(clf, X, y, n_splits=5, test_size=0.2, custom_classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9912280701754386\n",
      "0.9649122807017544\n",
      "0.9473684210526315\n",
      "0.9385964912280702\n",
      "0.956140350877193\n",
      "average score: 0.9596491228070174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9596491228070174"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf_sklearn_svm = svm.SVC(kernel=\"linear\", C=1).fit(X_train, y_train)\n",
    "cross_validation(clf_sklearn_svm, X, y, n_splits=5, test_size=0.2, custom_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00c7bf354c56290f5c8a36532e89f48915b186527b8684a9ec4204cb0b599f8c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
